{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vTqhoQCnqvtH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2529a0-bac0-4145-d6b0-75691bd2aca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ─── Setup ─────────────────────────────────────────────────────────────────────\n",
        "!pip install diffusers transformers accelerate --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.set_default_dtype(torch.float16)\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\"\n",
        ").to(device,dtype=torch.float16)\n",
        "vae.eval()\n",
        "\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(device,dtype=torch.float16)\n",
        "unet.eval()\n",
        "\n",
        "scheduler = DDPMScheduler.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"\n",
        ")\n",
        "\n",
        "tokenizer    = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text_encoder = BertModel.from_pretrained(\"bert-base-uncased\").to(\"cpu\",dtype=torch.float16)\n",
        "text_encoder.eval()\n"
      ],
      "metadata": {
        "id": "Bir8qCUkqz92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_images(\n",
        "    prompts: list[str],\n",
        "    steps: int = 50,\n",
        "    guidance_scale: float = 7.5,\n",
        "    resolution: int = 512,\n",
        ") -> list[Image.Image]:\n",
        "    batch = len(prompts)\n",
        "    toks = tokenizer(\n",
        "        prompts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=tokenizer.model_max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    cond_embeds = text_encoder(**toks).last_hidden_state\n",
        "    uncond_toks = tokenizer(\n",
        "        [\"\"]*batch,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=tokenizer.model_max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    uncond_embeds = text_encoder(**uncond_toks).last_hidden_state\n",
        "    del toks, uncond_toks\n",
        "    torch.cuda.empty_cache()\n",
        "    cond_embeds   = cond_embeds.to(device)\n",
        "    uncond_embeds = uncond_embeds.to(device)\n",
        "    encoder_states = torch.cat([uncond_embeds, cond_embeds], dim=0)\n",
        "    latents = torch.randn(\n",
        "        (batch, unet.config.in_channels, 64, 64),\n",
        "        device=device\n",
        "    ) * scheduler.init_noise_sigma\n",
        "    scheduler.set_timesteps(steps)\n",
        "    for t in scheduler.timesteps[:steps]:\n",
        "        lat_in = torch.cat([latents, latents], dim=0)\n",
        "        noise_pred = unet(lat_in, t, encoder_hidden_states=encoder_states).sample\n",
        "        uncond_pred, cond_pred = noise_pred.chunk(2)\n",
        "        guided = uncond_pred + guidance_scale * (cond_pred - uncond_pred)\n",
        "        latents = scheduler.step(guided, t, latents).prev_sample\n",
        "    latents = latents / vae.config.scaling_factor\n",
        "    with torch.no_grad():\n",
        "        images = vae.decode(latents).sample\n",
        "    images = (images / 2 + 0.5).clamp(0, 1)\n",
        "    out = []\n",
        "    for i in range(batch):\n",
        "        arr = (images[i].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "        out.append(Image.fromarray(arr))\n",
        "    return out"
      ],
      "metadata": {
        "id": "NACInlYhrDqw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"A forest during sunset\",\"An astronaut on a horse\"]\n",
        "images = generate_images(prompts, steps=50, guidance_scale=7.5,resolution=256)\n",
        "for img in images:\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "ZNYrdsh2r0rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DZDYGXj95nRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}